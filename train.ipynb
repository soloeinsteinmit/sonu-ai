{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c3421f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# The direct download link for the dataset\n",
    "url = \"https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/bwh3zbpkpv-1.zip\"\n",
    "\n",
    "# Define the local filename to save the downloaded file\n",
    "# You can change this if you want a different name for the zip file\n",
    "local_filename = \"dataset.zip\"\n",
    "\n",
    "# --- Option 1: Using requests library (Recommended) ---\n",
    "print(f\"Attempting to download {url} using requests...\")\n",
    "try:\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    print(f\"Successfully downloaded {local_filename}\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error during download: {e}\")\n",
    "    print(\"Please check the URL and your internet connection.\")\n",
    "\n",
    "# --- Unzipping the downloaded file ---\n",
    "print(f\"Attempting to unzip {local_filename}...\")\n",
    "try:\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(local_filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"./\") # Extract to the current directory\n",
    "    print(\"Successfully unzipped the dataset.\")\n",
    "\n",
    "    # Optional: List the contents of the current directory to see the extracted files\n",
    "    print(\"\\nContents of the current directory after extraction:\")\n",
    "    for item in os.listdir(\"./\"):\n",
    "        print(item)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {local_filename} not found. Download might have failed.\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"Error: {local_filename} is not a valid zip file or is corrupted.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during unzipping: {e}\")\n",
    "\n",
    "# Optional: Clean up the downloaded zip file after extraction\n",
    "# if os.path.exists(local_filename):\n",
    "#     os.remove(local_filename)\n",
    "#     print(f\"Cleaned up {local_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd5dbb7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the filename of the zip file to delete\n",
    "zip_filename = \"dataset.zip\"\n",
    "\n",
    "print(f\"Attempting to delete {zip_filename}...\")\n",
    "\n",
    "try:\n",
    "    if os.path.exists(zip_filename):\n",
    "        os.remove(zip_filename)\n",
    "        print(f\"Successfully deleted {zip_filename}.\")\n",
    "    else:\n",
    "        print(f\"Warning: {zip_filename} not found, nothing to delete.\")\n",
    "except OSError as e:\n",
    "    print(f\"Error deleting {zip_filename}: {e}\")\n",
    "    print(\"Please check if the file is in use or if you have the necessary permissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a91c2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "AgriSentry AI Hybrid Training Script - IMPROVED VERSION\n",
    "Fixes overfitting issues and adds proper early stopping\n",
    "Optimized for H100 GPU with torch.compile, AMP, and increased data throughput.\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import re\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import colorama\n",
    "from colorama import Fore, Style\n",
    "\n",
    "print(\"✅ All imports successful!\")\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET AND PERFORMANCE CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Dataset configuration based on CCMT dataset structure\n",
    "CROPS = ['Cashew', 'Cassava', 'Maize', 'Tomato']\n",
    "DISEASES = {\n",
    "    'Cashew': ['anthracnose', 'gumosis', 'healthy', 'leaf miner', 'red rust'],\n",
    "    'Cassava': ['bacterial blight', 'brown spot', 'green mite', 'healthy', 'mosaic'],\n",
    "    'Maize': ['fall armyworm', 'grasshoper', 'healthy', 'leaf beetle', 'leaf blight', 'leaf spot', 'streak virus'],\n",
    "    'Tomato': ['healthy', 'leaf blight', 'leaf curl', 'septoria leaf spot', 'verticulium wilt']\n",
    "}\n",
    "\n",
    "# 🚀 PERFORMANCE-TUNED PARAMETERS\n",
    "BATCH_SIZE = 512\n",
    "NUM_WORKERS = 8\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "print(f\"📊 Dataset Configuration:\")\n",
    "print(f\"   Crops: {len(CROPS)}\")\n",
    "print(f\"   Total Disease Classes: {sum(len(diseases) for diseases in DISEASES.values())}\")\n",
    "print(f\"   Image Size: {IMG_SIZE}\")\n",
    "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   Num Workers: {NUM_WORKERS}\")\n",
    "\n",
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def clean_disease_name(disease_folder_name):\n",
    "    \"\"\"Remove numbers from disease folder names\"\"\"\n",
    "    cleaned = re.sub(r'\\d+$', '', disease_folder_name)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def get_image_files(directory):\n",
    "    \"\"\"Get all image files from a directory, handling multiple extensions.\"\"\"\n",
    "    extensions = [\"*.jpg\", \"*.JPG\", \"*.jpeg\", \"*.JPEG\"]\n",
    "    files = []\n",
    "    for ext in extensions:\n",
    "        files.extend(list(directory.glob(ext)))\n",
    "    return files\n",
    "\n",
    "# =============================================================================\n",
    "# PYTORCH DATASET CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class AgriSentryDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for AgriSentry AI\"\"\"\n",
    "    def __init__(self, filepaths, labels, transform=None):\n",
    "        self.filepaths = filepaths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filepath = self.filepaths[idx]\n",
    "        label = self.labels[idx]\n",
    "        try:\n",
    "            image = Image.open(filepath).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load image {filepath}. Skipping. Error: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# =============================================================================\n",
    "# DEVICE UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dataloader, device):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to the device\"\"\"\n",
    "        for b in self.dataloader:\n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dataloader)\n",
    "\n",
    "# =============================================================================\n",
    "# IMPROVED CNN MODEL ARCHITECTURE WITH DROPOUT\n",
    "# =============================================================================\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    \"\"\"Calculate accuracy\"\"\"\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        acc = accuracy(out, labels)\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "\n",
    "def ConvBlock(in_channels, out_channels, pool=False, dropout=0.0):\n",
    "    \"\"\"Convolution block with BatchNormalization and optional dropout\"\"\"\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "              nn.BatchNorm2d(out_channels),\n",
    "              nn.ReLU(inplace=True)]\n",
    "    \n",
    "    if dropout > 0:\n",
    "        layers.append(nn.Dropout2d(dropout))\n",
    "    \n",
    "    if pool:\n",
    "        layers.append(nn.MaxPool2d(4))\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class CNN_NeuralNet(ImageClassificationBase):\n",
    "    \"\"\"IMPROVED CNN Architecture for AgriSentry AI with Dropout Regularization\"\"\"\n",
    "    def __init__(self, in_channels, num_diseases, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Convolutional layers with dropout\n",
    "        self.conv1 = ConvBlock(in_channels, 64, dropout=0.1)\n",
    "        self.conv2 = ConvBlock(64, 128, pool=True, dropout=0.2)\n",
    "        self.res1 = nn.Sequential(\n",
    "            ConvBlock(128, 128, dropout=0.2), \n",
    "            ConvBlock(128, 128, dropout=0.2)\n",
    "        )\n",
    "        \n",
    "        self.conv3 = ConvBlock(128, 256, pool=True, dropout=0.3)\n",
    "        self.conv4 = ConvBlock(256, 512, pool=True, dropout=0.3)\n",
    "        self.res2 = nn.Sequential(\n",
    "            ConvBlock(512, 512, dropout=0.3), \n",
    "            ConvBlock(512, 512, dropout=0.3)\n",
    "        )\n",
    "        \n",
    "        # Improved classifier with dropout\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(dropout_rate * 0.6),  # Slightly less dropout in middle\n",
    "            nn.Linear(256, num_diseases)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET PROCESSOR\n",
    "# =============================================================================\n",
    "\n",
    "class CCMTDatasetProcessor:\n",
    "    \"\"\"Processes the CCMT dataset for AgriSentry AI model training.\"\"\"\n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        if not self.dataset_path.exists():\n",
    "            raise FileNotFoundError(f\"Dataset directory not found at: {self.dataset_path}\")\n",
    "        self.crops = CROPS\n",
    "        self.diseases = DISEASES\n",
    "        self.img_size = IMG_SIZE\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.class_names = self._create_class_mappings()\n",
    "        self.num_classes = len(self.class_names)\n",
    "        print(f\"✅ Dataset processor initialized for {self.num_classes} classes.\")\n",
    "\n",
    "    def _create_class_mappings(self):\n",
    "        \"\"\"Create unified class mappings across all crops\"\"\"\n",
    "        return sorted([f\"{crop}_{disease}\" for crop in self.crops for disease in self.diseases[crop]])\n",
    "\n",
    "    def create_data_loaders(self, device):\n",
    "        \"\"\"Create PyTorch DataLoaders for training and validation with improved augmentation\"\"\"\n",
    "        print(\"🔄 Creating Data Loaders...\")\n",
    "        \n",
    "        # IMPROVED: More conservative augmentation to prevent overfitting\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize(self.img_size),\n",
    "            transforms.RandomRotation(10),  # Reduced from 20\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),  # Reduced\n",
    "            transforms.RandomResizedCrop(self.img_size[0], scale=(0.8, 1.0)),  # Added\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize(self.img_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        train_filepaths, train_labels, test_filepaths, test_labels = [], [], [], []\n",
    "        \n",
    "        for crop in self.crops:\n",
    "            for subset in [\"train_set\", \"test_set\"]:\n",
    "                subset_path = self.dataset_path / crop / subset\n",
    "                if not subset_path.exists(): \n",
    "                    continue\n",
    "                    \n",
    "                for disease_folder in [d for d in subset_path.iterdir() if d.is_dir()]:\n",
    "                    disease_name = clean_disease_name(disease_folder.name)\n",
    "                    class_name = f\"{crop}_{disease_name}\"\n",
    "                    \n",
    "                    if class_name not in self.class_names:\n",
    "                        continue\n",
    "                        \n",
    "                    class_idx = self.class_names.index(class_name)\n",
    "                    \n",
    "                    for f in get_image_files(disease_folder):\n",
    "                        if subset == \"train_set\":\n",
    "                            train_filepaths.append(str(f))\n",
    "                            train_labels.append(class_idx)\n",
    "                        else:\n",
    "                            test_filepaths.append(str(f))\n",
    "                            test_labels.append(class_idx)\n",
    "\n",
    "        print(f\"📊 Total images found: {len(train_filepaths)} train, {len(test_filepaths)} test\")\n",
    "        \n",
    "        train_dataset = AgriSentryDataset(train_filepaths, train_labels, transform=train_transform)\n",
    "        test_dataset = AgriSentryDataset(test_filepaths, test_labels, transform=val_transform)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True,\n",
    "                                  num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False,\n",
    "                                 num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "        train_loader = DeviceDataLoader(train_loader, device)\n",
    "        test_loader = DeviceDataLoader(test_loader, device)\n",
    "        print(f\"✅ DataLoaders created.\")\n",
    "        return train_loader, test_loader\n",
    "\n",
    "# =============================================================================\n",
    "# IMPROVED TRAINING FUNCTIONS WITH EARLY STOPPING\n",
    "# =============================================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit_with_early_stopping(epochs, max_lr, model, train_loader, val_loader, \n",
    "                           weight_decay=0, grad_clip=None, opt_func=torch.optim.Adam, \n",
    "                           patience=7, min_delta=0.001):\n",
    "    \"\"\"\n",
    "    IMPROVED: Training function with proper early stopping and better learning rate scheduling\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    # Use fused optimizer for CUDA & initialize GradScaler for AMP\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay, fused=True)\n",
    "    \n",
    "    # IMPROVED: Use ReduceLROnPlateau instead of OneCycleLR for better stability\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    print(f\"🚀 Starting training with patience={patience}, min_delta={min_delta}\")\n",
    "    print(f\"📊 Initial learning rate: {max_lr}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            \n",
    "            # Scale loss, step, and update\n",
    "            scaler.scale(loss).backward()\n",
    "            if grad_clip:\n",
    "                scaler.unscale_(optimizer)\n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lr'] = get_lr(optimizer)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(result['val_loss'])\n",
    "        \n",
    "        # Display progress\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "        \n",
    "        # Early stopping logic\n",
    "        improved = False\n",
    "        if result['val_loss'] < (best_val_loss - min_delta):\n",
    "            best_val_loss = result['val_loss']\n",
    "            best_val_acc = result['val_acc']\n",
    "            patience_counter = 0\n",
    "            improved = True\n",
    "            \n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': result['val_loss'],\n",
    "                'val_acc': result['val_acc'],\n",
    "            }, 'best_model.pth')\n",
    "            print(f\"   ✅ New best model saved! Val Loss: {best_val_loss:.4f}, Val Acc: {best_val_acc:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        print(f\"   📊 Patience: {patience_counter}/{patience}, Best Val Acc: {best_val_acc:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\n🛑 Early stopping triggered after {epoch + 1} epochs\")\n",
    "            print(f\"   Best validation loss: {best_val_loss:.4f}\")\n",
    "            print(f\"   Best validation accuracy: {best_val_acc:.4f}\")\n",
    "            break\n",
    "            \n",
    "        # Stop if learning rate becomes too small\n",
    "        if get_lr(optimizer) < 1e-6:\n",
    "            print(f\"\\n🛑 Learning rate too small, stopping training\")\n",
    "            break\n",
    "\n",
    "    return history\n",
    "\n",
    "def plot_training_history(history, output_dir):\n",
    "    \"\"\"Enhanced plotting function with more details\"\"\"\n",
    "    if not history:\n",
    "        print(\"No history to plot\")\n",
    "        return\n",
    "        \n",
    "    epochs = range(len(history))\n",
    "    val_acc = [x['val_acc'] for x in history]\n",
    "    val_loss = [x['val_loss'] for x in history]\n",
    "    train_loss = [x.get('train_loss', 0) for x in history]\n",
    "    learning_rates = [x.get('lr', 0) for x in history]\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(epochs, train_loss, 'r--', label='Training Loss', linewidth=2)\n",
    "    ax1.plot(epochs, val_loss, 'g-', label='Validation Loss', linewidth=2)\n",
    "    ax1.set_title('Training and Validation Loss', fontsize=14)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax2.plot(epochs, val_acc, 'b-', label='Validation Accuracy', linewidth=2)\n",
    "    ax2.set_title('Validation Accuracy', fontsize=14)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate plot\n",
    "    ax3.plot(epochs, learning_rates, 'orange', label='Learning Rate', linewidth=2)\n",
    "    ax3.set_title('Learning Rate Schedule', fontsize=14)\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss difference plot\n",
    "    loss_diff = [val - train for val, train in zip(val_loss, train_loss)]\n",
    "    ax4.plot(epochs, loss_diff, 'purple', label='Val Loss - Train Loss', linewidth=2)\n",
    "    ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax4.set_title('Overfitting Monitor (Val - Train Loss)', fontsize=14)\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Loss Difference')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = Path(output_dir) / 'training_history_improved.png'\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"✅ Training history saved to: {save_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TRAINING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training pipeline with improvements\"\"\"\n",
    "    print(\"🚀 AgriSentry AI IMPROVED Training Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # --- Configuration ---\n",
    "    DATASET_PATH = \"Dataset for Crop Pest and Disease Detection/CCMT Dataset-Augmented\"\n",
    "    OUTPUT_DIR = \"agrisentry_pytorch_output_improved\"\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    device = get_default_device()\n",
    "    print(f\"🖥️  Using device: {device}\")\n",
    "    \n",
    "    # --- Data Processing ---\n",
    "    processor = CCMTDatasetProcessor(DATASET_PATH)\n",
    "    train_loader, test_loader = processor.create_data_loaders(device)\n",
    "    \n",
    "    # --- Model Building and Compilation ---\n",
    "    print(\"\\n🏗️  Building and Compiling Model...\")\n",
    "    model = to_device(CNN_NeuralNet(3, processor.num_classes, dropout_rate=0.5), device)\n",
    "    print(f\"   Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Compile the model for speedups\n",
    "    print(\"   Compiling model with torch.compile()...\")\n",
    "    model = torch.compile(model)\n",
    "    print(\"✅ Model compiled!\")\n",
    "    \n",
    "    # --- IMPROVED Training Parameters ---\n",
    "    print(\"\\n🎯 Training Configuration:\")\n",
    "    num_epochs = 100\n",
    "    max_lr = 0.003  # REDUCED from 0.01\n",
    "    grad_clip = 0.1\n",
    "    weight_decay = 1e-3  # INCREASED from 1e-4\n",
    "    patience = 10  # INCREASED for better training\n",
    "    min_delta = 0.001  # Minimum improvement threshold\n",
    "    \n",
    "    print(f\"   Max epochs: {num_epochs}\")\n",
    "    print(f\"   Learning rate: {max_lr}\")\n",
    "    print(f\"   Weight decay: {weight_decay}\")\n",
    "    print(f\"   Patience: {patience}\")\n",
    "    print(f\"   Min delta: {min_delta}\")\n",
    "    \n",
    "    # --- Training ---\n",
    "    print(\"\\n🚀 Starting Training...\")\n",
    "    history = fit_with_early_stopping(\n",
    "        num_epochs, max_lr, model, train_loader, test_loader,\n",
    "        grad_clip=grad_clip, weight_decay=weight_decay, \n",
    "        opt_func=torch.optim.Adam, patience=patience, min_delta=min_delta\n",
    "    )\n",
    "    \n",
    "    # --- Load Best Model ---\n",
    "    print(\"\\n📥 Loading best model...\")\n",
    "    if Path('best_model.pth').exists():\n",
    "        checkpoint = torch.load('best_model.pth')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"✅ Best model loaded from epoch {checkpoint['epoch']}\")\n",
    "        print(f\"   Final validation accuracy: {checkpoint['val_acc']:.4f}\")\n",
    "        print(f\"   Final validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "    \n",
    "    # --- Saving Final Model ---\n",
    "    print(\"\\n💾 Saving Final Model...\")\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_path = Path(OUTPUT_DIR) / f\"agrisentry_model_improved_{timestamp}.pth\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'class_names': processor.class_names,\n",
    "        'num_classes': processor.num_classes,\n",
    "        'history': history,\n",
    "        'hyperparameters': {\n",
    "            'max_lr': max_lr,\n",
    "            'weight_decay': weight_decay,\n",
    "            'patience': patience,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'dropout_rate': 0.5\n",
    "        }\n",
    "    }, model_path)\n",
    "    print(f\"✅ Final model saved to: {model_path}\")\n",
    "    \n",
    "    # --- Plotting Results ---\n",
    "    print(\"\\n📊 Plotting Results...\")\n",
    "    plot_training_history(history, OUTPUT_DIR)\n",
    "    \n",
    "    # --- Final Summary ---\n",
    "    if history:\n",
    "        best_epoch = max(range(len(history)), key=lambda i: history[i]['val_acc'])\n",
    "        best_result = history[best_epoch]\n",
    "        print(f\"\\n🎉 Training Complete!\")\n",
    "        print(f\"   Total epochs: {len(history)}\")\n",
    "        print(f\"   Best epoch: {best_epoch}\")\n",
    "        print(f\"   Best validation accuracy: {best_result['val_acc']:.4f}\")\n",
    "        print(f\"   Best validation loss: {best_result['val_loss']:.4f}\")\n",
    "        print(f\"   Final learning rate: {history[-1].get('lr', 'N/A')}\")\n",
    "    \n",
    "    return model, history, processor\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, history, processor = main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
